{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattiaTarantino/Machine-Learning-project/blob/main/Fondamenti_di_IA_homework_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TspH1wJmAkdk"
      },
      "source": [
        "### For the homeworks we are going to use the \"[Online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#)\"\n",
        "\n",
        "The dataset can be used both for regression and classification tasks.\n",
        "\n",
        "#### Source:\n",
        "\n",
        "Kelwin Fernandes INESC TEC, Porto, Portugal/Universidade do Porto, Portugal.\n",
        "Pedro Vinagre ALGORITMI Research Centre, Universidade do Minho, Portugal\n",
        "Paulo Cortez ALGORITMI Research Centre, Universidade do Minho, Portugal\n",
        "Pedro Sernadela Universidade de Aveiro\n",
        "\n",
        "#### Data Set Information:\n",
        "\n",
        "* The articles were published by Mashable (www.mashable.com) and their content as the rights to reproduce it belongs to them. Hence, this dataset does not share the original content but some statistics associated with it. The original content be publicly accessed and retrieved using the provided urls.\n",
        "* Acquisition date: January 8, 2015\n",
        "* The estimated relative performance values were estimated by the authors using a Random Forest classifier and a rolling windows as assessment method. See their article for more details on how the relative performance values were set.\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field)\n",
        "\n",
        "Attribute Information:\n",
        "0. url: URL of the article (non-predictive)\n",
        "1. timedelta: Days between the article publication and the dataset acquisition (non-predictive)\n",
        "2. n_tokens_title: Number of words in the title\n",
        "3. n_tokens_content: Number of words in the content\n",
        "4. n_unique_tokens: Rate of unique words in the content\n",
        "5. n_non_stop_words: Rate of non-stop words in the content\n",
        "6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
        "7. num_hrefs: Number of links\n",
        "8. num_self_hrefs: Number of links to other articles published by Mashable\n",
        "9. num_imgs: Number of images\n",
        "10. num_videos: Number of videos\n",
        "11. average_token_length: Average length of the words in the content\n",
        "12. num_keywords: Number of keywords in the metadata\n",
        "13. data_channel_is_lifestyle: Is data channel 'Lifestyle'?\n",
        "14. data_channel_is_entertainment: Is data channel 'Entertainment'?\n",
        "15. data_channel_is_bus: Is data channel 'Business'?\n",
        "16. data_channel_is_socmed: Is data channel 'Social Media'?\n",
        "17. data_channel_is_tech: Is data channel 'Tech'?\n",
        "18. data_channel_is_world: Is data channel 'World'?\n",
        "19. kw_min_min: Worst keyword (min. shares)\n",
        "20. kw_max_min: Worst keyword (max. shares)\n",
        "21. kw_avg_min: Worst keyword (avg. shares)\n",
        "22. kw_min_max: Best keyword (min. shares)\n",
        "23. kw_max_max: Best keyword (max. shares)\n",
        "24. kw_avg_max: Best keyword (avg. shares)\n",
        "25. kw_min_avg: Avg. keyword (min. shares)\n",
        "26. kw_max_avg: Avg. keyword (max. shares)\n",
        "27. kw_avg_avg: Avg. keyword (avg. shares)\n",
        "28. self_reference_min_shares: Min. shares of referenced articles in Mashable\n",
        "29. self_reference_max_shares: Max. shares of referenced articles in Mashable\n",
        "30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable\n",
        "31. weekday_is_monday: Was the article published on a Monday?\n",
        "32. weekday_is_tuesday: Was the article published on a Tuesday?\n",
        "33. weekday_is_wednesday: Was the article published on a Wednesday?\n",
        "34. weekday_is_thursday: Was the article published on a Thursday?\n",
        "35. weekday_is_friday: Was the article published on a Friday?\n",
        "36. weekday_is_saturday: Was the article published on a Saturday?\n",
        "37. weekday_is_sunday: Was the article published on a Sunday?\n",
        "38. is_weekend: Was the article published on the weekend?\n",
        "39. LDA_00: Closeness to LDA topic 0\n",
        "40. LDA_01: Closeness to LDA topic 1\n",
        "41. LDA_02: Closeness to LDA topic 2\n",
        "42. LDA_03: Closeness to LDA topic 3\n",
        "43. LDA_04: Closeness to LDA topic 4\n",
        "44. global_subjectivity: Text subjectivity\n",
        "45. global_sentiment_polarity: Text sentiment polarity\n",
        "46. global_rate_positive_words: Rate of positive words in the content\n",
        "47. global_rate_negative_words: Rate of negative words in the content\n",
        "48. rate_positive_words: Rate of positive words among non-neutral tokens\n",
        "49. rate_negative_words: Rate of negative words among non-neutral tokens\n",
        "50. avg_positive_polarity: Avg. polarity of positive words\n",
        "51. min_positive_polarity: Min. polarity of positive words\n",
        "52. max_positive_polarity: Max. polarity of positive words\n",
        "53. avg_negative_polarity: Avg. polarity of negative words\n",
        "54. min_negative_polarity: Min. polarity of negative words\n",
        "55. max_negative_polarity: Max. polarity of negative words\n",
        "56. title_subjectivity: Title subjectivity\n",
        "57. title_sentiment_polarity: Title polarity\n",
        "58. abs_title_subjectivity: Absolute subjectivity level\n",
        "59. abs_title_sentiment_polarity: Absolute polarity level\n",
        "60. shares: Number of shares (target)\n",
        "\n",
        "\n",
        "The first two columns (url and time_delta) are non-predictive and should be ignored\n",
        "\n",
        "The last column **shares** contains the value to predict.\n",
        "\n",
        "### Regression\n",
        "In the case of regression we want to predict the value of the share column.\n",
        "\n",
        "### Classification\n",
        "In the case of classification we want to predict one of two classes:\n",
        "\n",
        "* *low* -- shares < 1,400\n",
        "* *high* -- shares >= 1,400\n",
        "\n",
        "### Metrics\n",
        "\n",
        "#### Regression\n",
        "To evaluate how good we are doing on the **regression** task we will use the Root Mean Squared Error (RMSE). RMSE is given by\n",
        "\n",
        "$$\n",
        "\\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{n}{\\Big(d_i -f_i\\Big)^2}}\n",
        "$$\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "* $n$ is the number of test samples\n",
        "* $d_i$ is the ground truth value of the i-th sample\n",
        "* $f_i$ is the predicted value of the i-th sample\n",
        "\n",
        "\n",
        "#### Classification\n",
        "To evaluate how good we are doing on the **classification** task we will use the accuracy metrics. Accuracy is given by\n",
        "\n",
        "$$\n",
        "\\frac{TP+TN}{TP+TN+FP+FN}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* TP is the number of *correctly* classified positive samples\n",
        "* TN is the number of *correctly* classified negative samples\n",
        "* FP is the number of *incorrectly* classified positive samples\n",
        "* FN is the number of *incorrectly* classified negative samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oesd6_bYijRo"
      },
      "outputs": [],
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmad6QdZ_nFR"
      },
      "outputs": [],
      "source": [
        "!unzip OnlineNewsPopularity.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eW4t_c6ACcm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format properly the names of the columns and remove the first two columns"
      ],
      "metadata": {
        "id": "yXCX_LpFedtj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mxntjhmAH0D"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[: , 2:]\n",
        "# Adding column values to high(1) and low(0) in base of shares for classification \n",
        "df['class'] = np.where(df['shares'] >= 1400, 'high', 'low')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCXwyljnAMmi"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#  Classification \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vmWAIUmZbi6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node():\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None): \n",
        "        # for decision node\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.info_gain = info_gain\n",
        "        \n",
        "        # for leaf node\n",
        "        self.value = value"
      ],
      "metadata": {
        "id": "tx1mdyhTPfx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTreeClassifier():\n",
        "      def __init__(self, min_samples_split=2, max_depth=3):\n",
        "        \n",
        "        # initialize the root of the tree \n",
        "        self.root = None\n",
        "        \n",
        "        # stopping conditions\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "      def build_tree(self, dataset, curr_depth=0):\n",
        "        \n",
        "        x, y = dataset.iloc[: , :58], dataset['class']\n",
        "        num_samples, num_features = len(x.columns), len(x)\n",
        "        \n",
        "        # split until stopping conditions are met\n",
        "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
        "            # find the best split\n",
        "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
        "            # check if information gain is positive\n",
        "            if best_split[\"info_gain\"]>0:\n",
        "                # recur left\n",
        "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
        "                # recur right\n",
        "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
        "                # return decision node\n",
        "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
        "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
        "        \n",
        "        # compute leaf node\n",
        "        leaf_value = self.calculate_leaf_value(y)\n",
        "        # return leaf node\n",
        "        return Node(value=leaf_value)\n",
        "\n",
        "\n",
        "      # Funzione per calcolare l'entropia\n",
        "      def entropy(self, y):\n",
        "          h = 0\n",
        "          if len(y) > 0:\n",
        "            p = y.value_counts()[0]\n",
        "            n = y.value_counts()[1]\n",
        "            q = p/(p+n)\n",
        "            h = - (q*np.log2(q) + (1-q)*np.log2(1-q))\n",
        "          return h\n",
        "\n",
        "      # Funzione per dividere i dati \n",
        "      def split(self, df, feature_index, threshold):\n",
        "          df_left = df[df.iloc[:,feature_index] <= threshold]\n",
        "          df_right = df[df.iloc[:,feature_index] > threshold]\n",
        "          return df_left, df_right\n",
        "\n",
        "      # Funzione per calcolare l'information gain\n",
        "      def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
        "          weight_l = len(l_child) / len(parent)\n",
        "          weight_r = len(r_child) / len(parent)\n",
        "          #if mode==\"gini\":\n",
        "              #gain = gini_index(parent) - (weight_l*gini_index(l_child) + weight_r*gini_index(r_child))\n",
        "          #else:\n",
        "          gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
        "          return gain\n",
        "\n",
        "      def get_best_split(self, dataset, num_samples, num_features):\n",
        "          best_split = {}\n",
        "          max_info_gain = -float(\"inf\")\n",
        "          for feature_index in range(58):\n",
        "              feature_values = df.iloc[:,feature_index]\n",
        "              median = feature_values.median()\n",
        "              df_left, df_right = self.split(df, feature_index, median)\n",
        "              if len(df_left)>0 and len(df_right)>0:\n",
        "                  y, left_y, right_y = df.iloc[: , 59:], df_left.iloc[: , 59:], df_right.iloc[: , 59:]\n",
        "                  curr_info_gain = self.information_gain(y, left_y, right_y)\n",
        "                  if curr_info_gain>max_info_gain:\n",
        "                      best_split[\"feature_index\"] = feature_index\n",
        "                      best_split[\"threshold\"] = median\n",
        "                      best_split[\"dataset_left\"] = df_left\n",
        "                      best_split[\"dataset_right\"] = df_right\n",
        "                      best_split[\"info_gain\"] = curr_info_gain\n",
        "                      max_info_gain = curr_info_gain\n",
        "\n",
        "          return best_split\n",
        "\n",
        "      def calculate_leaf_value(self, Y):\n",
        "        Y = Y.tolist()\n",
        "        return max(Y, key=Y.count)\n",
        "\n",
        "      def fit(self, dataset):\n",
        "        self.root = self.build_tree(dataset)\n",
        "\n",
        "      def predict(self, X):\n",
        "        # Function to predict new dataset \n",
        "        preditions = []\n",
        "        for i in range(len(X)):\n",
        "          preditions.append(self.make_prediction(X.iloc[i,:], self.root))\n",
        "        return preditions\n",
        "\n",
        "      def make_prediction(self, x, tree):\n",
        "        # Function to predict a single data point\n",
        "        if tree.value != None:\n",
        "           return tree.value\n",
        "        feature_val = x[tree.feature_index]     \n",
        "        if feature_val <= tree.threshold:\n",
        "            return self.make_prediction(x, tree.left)\n",
        "        else:\n",
        "            return self.make_prediction(x, tree.right)\n",
        "\n"
      ],
      "metadata": {
        "id": "lmWzG06Z0J97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x, y = df.iloc[: , :58], df['class']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n"
      ],
      "metadata": {
        "id": "ca_zR3p-prTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=3)\n",
        "classifier.fit(df)"
      ],
      "metadata": {
        "id": "oio3sEck-XG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(x_test) \n",
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TJpvKv5-t9Q",
        "outputId": "0621830c-c23f-475d-91da-cc22aabd452b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 59.74271660991298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation using Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsZJMeoKKhRo",
        "outputId": "724eeca4-d6cb-4a05-f773-a51aba3160d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2519, 1726],\n",
              "       [1466, 2218]])"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#  Sklearn testing - Classification\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sFEMggQao-pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Samples and labels for classification\n",
        "x, y = df.iloc[: , :58], df['class']"
      ],
      "metadata": {
        "id": "EZ9AXGC7j8n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "ntzQNtL2QNkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create Decision Tree classifer object\n",
        "model = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 1)\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "model = model.fit(x_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "id": "UkUyfRApQoak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation using Accuracy score\n",
        "from sklearn import metrics\n",
        "\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N7kgY4gQ1Kb",
        "outputId": "b4630fe3-93db-461a-835d-3019c37c3dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 59.78055240257283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation using Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4B63NNtQ6uH",
        "outputId": "f9ea3898-dc62-41ba-c81a-f2588eac021d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2600, 1645],\n",
              "       [1605, 2079]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vmWAIUmZbi6G",
        "sFEMggQao-pt"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}